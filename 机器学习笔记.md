# 机器学习
- 监督学习
    - 回归
    - 分类
- 非监督学习
    - 聚类
    - 降维
 
## 线性回归模型
$$ f(x)=wx+b $$
## 代价函数  
$$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{n} (y^{(i)} - \hat{y}^{(i)})^2 $$  

## 梯度下降  
```plaintext
for  
    tmpw=w-alpha * d(J)/dw  
    tmpb=w-alpha * d(J)/db  
    b=tmpw b=tmpb  
```
- alpha为学习率
    - 学习率过小会浪费时间，学习率过大可能导致发散
- 注意当接近极小值时导数项会自动变小，更新步骤也会自动变小，达到局部最小时导数项为零

## batch gradient descent 批量线性回归
- 考虑所有训练样本而非训练集的子集

# 多类特征
- 行向量表示样本，列向量表示同一特征
- 上标表示样本，下标表示特征
## n个特征的线性回归模型
$$\[ f(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b \] $$
```python
        import numpy as np

        def func(x,w,b):
            return np.dot(x,w)+b
```
或
```python
for j in range(n):
    f+=x[j]*w[j]

f+=b
```
- 向量化并行计算，更加迅速
- $$w_j=w-alpha * d(J)/dw_j$$
- $$\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} ( \mathbf{w}^T \mathbf{x}^{(i)} + b - y^{(i)} ) \cdot x_j^{(i)}$$

**非向量化实现代码**

```python
# 初始化梯度
dj_dw = np.zeros((n,))  # 权重梯度向量 (n维)
dj_db = 0.0             # 偏置项梯度

# 计算梯度
for i in range(m):      # 遍历每个样本
    # 计算预测误差
    err = np.dot(w, X[i]) + b - y[i]
    
    # 计算权重梯度
    for j in range(n):  # 遍历每个特征
        dj_dw[j] = dj_dw[j] + err * X[i][j]  # 注意这里应该是X[i][j]不是w[j]
    
    # 计算偏置梯度
    dj_db = dj_db + err

# 平均梯度
dj_dw = dj_dw / m
dj_db = dj_db / m

# 参数更新
w = w - alpha * dj_dw   # alpha是学习率
b = b - alpha * dj_db
```

**更简单的向量化实现**
```python
# Vectorization implementation
err = np.dot(X, w) + b - y
dj_dw = np.dot(X.T, err) / m
dj_db = np.sum(err) / m




w = w - alpha * dj_dw
b = b - alpha * dj_db
```


## 特征缩放
- 当不同特征取值范围相差较远时，因为学习率可能并不普适于不同特征，所以可能导致梯度下降时发散
- 特征缩放可加速梯度下降
- How to feature scale
    - Divide by the range maximum
    - Mean normalization $$x^{(i)}=(x^{(i)}-mean)/(max-min)$$

