# 机器学习
- 监督学习
    - 回归
    - 分类
- 非监督学习
    - 聚类
    - 降维
 
## 线性回归模型
$$ f(x)=wx+b $$
## 代价函数  
$$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{n} (y^{(i)} - \hat{y}^{(i)})^2 $$  

## 梯度下降  
```plaintext
for  
    tmpw=w-alpha * d(J)/dw  
    tmpb=w-alpha * d(J)/db  
    b=tmpw b=tmpb  
```
- alpha为学习率
    - 学习率过小会浪费时间，学习率过大可能导致发散
- 注意当接近极小值时导数项会自动变小，更新步骤也会自动变小，达到局部最小时导数项为零

## batch gradient descent 批量线性回归
- 考虑所有训练样本而非训练集的子集

# 多类特征
- 行向量表示样本，列向量表示同一特征
- 上标表示样本，下标表示特征
## n个特征的线性回归模型
$$\[ f(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b \] $$
```python
        import numpy as np

        def func(x,w,b):
            return np.dot(x,w)+b
```
或
```python
for j in range(n):
    f+=x[j]*w[j]

f+=b
```
- 向量化并行计算，更加迅速
- $$w_j=w-alpha * d(J)/dw_j$$
- $$\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} ( \mathbf{w}^T \mathbf{x}^{(i)} + b - y^{(i)} ) \cdot x_j^{(i)}$$



