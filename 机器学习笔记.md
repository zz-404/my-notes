# 机器学习
- 监督学习
    - 回归
    - 分类
- 非监督学习
    - 聚类
    - 降维
 
## 线性回归模型
$$ f(x)=wx+b $$
## 代价函数  
$$ J(w,b) = \frac{1}{2m} \sum_{i=1}^{n} (y^{(i)} - \hat{y}^{(i)})^2 $$  
```python
def computeCost(X, y, w):
    m = X.shape[0]  # 样本数量
    predictions = X @ w
    cost = np.sum(np.power(predictions - y, 2)) / (2 * m)
    return cost
```
## 梯度下降  
```plaintext
for  
    tmpw=w-alpha * d(J)/dw  
    tmpb=w-alpha * d(J)/db  
    b=tmpw b=tmpb  
```
- alpha为学习率
    - 学习率过小会浪费时间，学习率过大可能导致发散
- 注意当接近极小值时导数项会自动变小，更新步骤也会自动变小，达到局部最小时导数项为零

## batch gradient descent 批量线性回归
- 考虑所有训练样本而非训练集的子集

## 多类特征
- 行向量表示样本，列向量表示同一特征
- 上标表示样本，下标表示特征
## n个特征的线性回归模型
$$\[ f(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b \] $$
```python
        import numpy as np

        def func(x,w,b):
            return np.dot(x,w)+b
```
或
```python
for j in range(n):
    f+=x[j]*w[j]

f+=b
```
- 向量化并行计算，更加迅速
- $$w_j=w-alpha * d(J)/dw_j$$
- $$\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} ( \mathbf{w}^T \mathbf{x}^{(i)} + b - y^{(i)} ) \cdot x_j^{(i)}$$

**非向量化实现代码**

```python
# 初始化梯度
dj_dw = np.zeros((n,))  # 权重梯度向量 (n维)
dj_db = 0.0             # 偏置项梯度

# 计算梯度
for i in range(m):      # 遍历每个样本
    # 计算预测误差
    err = np.dot(w, X[i]) + b - y[i]
    
    # 计算权重梯度
    for j in range(n):  # 遍历每个特征
        dj_dw[j] = dj_dw[j] + err * X[i][j]  # 注意这里应该是X[i][j]不是w[j]
    
    # 计算偏置梯度
    dj_db = dj_db + err

# 平均梯度
dj_dw = dj_dw / m
dj_db = dj_db / m

# 参数更新
w = w - alpha * dj_dw   # alpha是学习率
b = b - alpha * dj_db
```

**更简单的向量化实现**
```python
# Vectorization implementation
err = np.dot(X, w) + b - y
dj_dw = np.dot(X.T, err) / m
dj_db = np.sum(err) / m

w = w - alpha * dj_dw
b = b - alpha * dj_db
```


## 特征缩放
- 当不同特征取值范围相差较远时，因为学习率可能并不普适于不同特征，所以可能导致梯度下降时发散
- 特征缩放可加速梯度下降
- How to feature scale
    - 除以范围最大值
    - 均值归一化 $$x^{(i)}=(x^{(i)}-mean)/(max-min)$$
    - z_score 归一化
        - 得到标准差，标准差公式 a=sqrt(sigma[(xi-mean)^2]/m)
        - 归一化 $$x^{(i)}=(x^{(i)}-mean)/(sigma)$$


## 检查梯度下降是否收敛
- 使用学习曲线（由代价函数和迭代次数为轴构成）
    - 当迭代之后代价函数反而升高，可能是学习率出现了问题或代码有误
    - 当学习曲线趋于平缓时可能梯度下降已接近收敛
- 使用自动收敛测试（设置一个极小值，当一次迭代后代价函数的减少小于该极小值epsilon则判定为收敛，但合适的epsilon选择极为困难）

### 选择良好的学习率
通过学习曲线选择  
可以找一个小值找一个大值然后不断缩小区间
- 良好的学习率应当足够小，且每次迭代都能够降低代价函数，但如果非常小仍不满足则应修正代码

### 多项式回归
根据特征选择合适的曲线，注意二次函数会回落，同时当应用高次幂时特征缩放变得更加重要
```python
x=x.reshape(-1,1)  #-1表示自动计算该维度大小
# np.arange(start, stop, step, dtype=None)函数 自动创建等差数列
```

# 逻辑回归
广泛用于分类
## sigmoid函数
Sigmoid 函数的公式为：

$$
g(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
$$
```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

  
**选择损失函数的核心是要使它成为一个关于参数的凸函数**
## 逻辑回归的代价函数定义为：
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)})) \right]
$$

其中：
- $h_\theta(x)$ 是sigmoid函数
- $m$ 是训练样本数量
- $y^{(i)}$ 是第i个样本的真实标签（0或1）
- $x^{(i)}$ 是第i个样本的特征向量

平方误差函数会惩罚过于正确的预测，且代价函数会成为非凸函数，同时出现多个局部极小值


## Python实现

```python
import numpy as np

def sigmoid(z):
    """Sigmoid函数"""
    return 1 / (1 + np.exp(-z))

def cost(theta, X, y):
    theta = np.matrix(theta)  # 转换为矩阵形式
    X = np.matrix(X)
    y = np.matrix(y)
    
    # 计算两部分损失
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    
    return np.sum(first - second) / len(X)
```
向量化实现
```
def vectorized_cost(theta, X, y):
    """向量化实现"""
    h = sigmoid(X @ theta+b)
    return (-y @ np.log(h) - (1-y) @ np.log(1-h)) / len(X)
```


## 过拟合
模型在训练集上表现很好，但在新数据（测试集）上泛化能力差
**解决过拟合**
- 增加训练数据
- 筛选重要特征（太多无关特征可能增加复杂度和噪声）
- 正则化（鼓励学习算法缩小参数值而非直接设置为零，保留所有特征时避免过大的影响）

