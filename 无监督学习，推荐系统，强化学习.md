# 无监督学习
- 聚类
- 异常检测

## 2.1 什么是聚类
- **定义**: 将相似的数据点自动分组到簇中的无监督学习算法
- **特点**: 
  - 无标签数据
  - 寻找数据内在的分组结构
- **应用场景**:
  - 市场细分
  - 社交网络分析
  - 组织计算集群
  - 天文数据分析

## 2.2 K-means的直观理解
- **核心思想**: 
  - 随机初始化K个聚类中心  注意是随机！！！  
  - 迭代执行两个步骤:
    1. 分配步骤: 将每个点分配到最近的聚类中心
    2. 移动步骤: 将聚类中心移动到该簇所有点的均值位置
- **可视化**: 通过不断迭代，聚类中心逐渐移动到簇的"中心"位置

## 2.3 K-means算法
- **算法步骤**:
  1. 随机初始化K个聚类中心 μ₁, μ₂, ..., μₖ
  2. 重复直到收敛:
     - 对每个样本i，计算其所属簇: c⁽ⁱ⁾ = argminⱼ‖x⁽ⁱ⁾ - μⱼ‖²   (L2距离/欧几里得距离)
     **c⁽ⁱ⁾是该样本所分给类的编号，从1到k **
     - 对每个簇j，重新计算中心: μⱼ = (∑c⁽ⁱ⁾=j x⁽ⁱ⁾) / (c⁽ⁱ⁾=j的样本数)
    
  如果没有任何一个样本分配给某个类，直接删除该类，或重新初始化确保每个聚类都能分配到一些样本 

## 2.4 优化目标
- **代价函数**(失真函数): 
  J(c⁽¹⁾,...,c⁽ᵐ⁾, μ₁,...,μₖ) = (1/m)∑ᵢ₌₁ᵐ‖x⁽ⁱ⁾ - μ_c⁽ⁱ⁾‖²
  
  **注意J是距离函数的平方，这样做可以更好的反应聚类中心是否在整个簇的中心位置，如果只是不加平方的距离的话则无法确保在中心**  
- **优化目标**: 最小化代价函数J
- **算法保证**: 
  - 分配步骤: 固定μ，优化c，降低J
  - 移动步骤: 固定c，优化μ，降低J
  - 每次迭代都保证J单调递减  
成本函数停止下降或下降小于某个阈值时则已经收敛，成本函数绝对不可能上升

## 2.5 初始化K-means
- **随机初始化方法**:
  1. 从训练集中随机选择K(<m)个样本作为初始聚类中心
  2. 通常需要多次随机初始化(50-1000次)
  3. 选择代价函数J最小的结果(**因为使用了平方，所以J反映了类的集中程度**)
- **K值选择问题**: 随机初始化可能导致局部最优解

## 2.6 选择聚类的个数
- **肘部法则**(Elbow Method):
  - 绘制K值与代价函数J的关系曲线
  - 选择曲线"肘部"对应的K值(拐点)
  - **注意只选择使J最小的K是不起作用的，因为K越大成本函数越小（点越集中），所以只会选到那个最大的K值**
- **实际考虑**:
  - 肘部法则可能不明显
  - 最终选择应该服务于后续目的
  - 根据不同需求评估不同K值的表现

## 第三部分 异常检测与推荐系统

### 3.1 发现异常事件
- **异常检测定义**: 识别与正常模式显著不同的数据点，计算分布在某个区域的概率，小于ε标记为异常
- 
- **应用场景**:
  - 欺诈检测
  - 工业制造缺陷检测
  - 数据中心监控异常机器
  - 医疗异常检测

### 3.2 高斯（正态）分布
- **公式**: $p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$
**μ是均值，σ是标准差（方差再开平方）**
- **参数估计**:
  - $\mu = \frac{1}{m}\sum_{i=1}^m x^{(i)}$
  - $\sigma^2 = \frac{1}{m}\sum_{i=1}^m (x^{(i)}-\mu)^2$

### 3.3 异常检测算法
- **步骤**:
  1. 选择可能表示异常的特征 $x_i$
  2. 计算参数 $\mu_1,...,\mu_n,\sigma_1^2,...,\sigma_n^2$
  3. 对新样本 $x$ ，计算 $p(x) = \prod_{j=1}^n p(x_j;\mu_j,\sigma_j^2)$
  4. 如果 $p(x) < \varepsilon$ ，则判定为异常

### 3.4 开发和评估异常检测系统  

实数评价：得到一个数字来告诉你算法变好了还是变差了  

- **评估方法**:
  - 使用带标签数据（正常/异常）
  - 划分训练集（**仅正常样本！！！**）、交叉验证集、测试集，（或者另一种算法直接不使用测试集，因为异常样本太少了 ）
  - 使用F1-score、精确率、召回率等指标
根据评估结果不断调整你的ε和其他参数

### 3.5 异常检测 vs. 监督学习
- **异常检测适用场景**:
  - 异常样本数量很少（0-20）
  - 异常类型多样或从未见过，难以从正样本学习
- **监督学习适用场景**:
  - 正负样本数量都很大
  - 有足够多的异常样本用于训练

### 3.6 选择要使用的特征
- **特征转换**: 对非高斯分布特征进行转换（如log(x+c), x^c）
- **误差分析**:
  - 利用直觉，找出那些虽然算法无法区分但我们仍然认为它是异常的样本与其他样本的不同，添加新特征帮助区分异常值
  - 找出那些异常样本中表现非常大或者非常小的特征，作为算法的新特征
  - 通过数学处理创建新特征
**注意添加新特征时要始终保持正例p大，异常p小**
- **多元高斯分布**: 可捕捉特征间的关系

## 第四部分 推荐系统基础

### 4.1 推荐系统
- **推荐系统应用**: 电影推荐、产品推荐、内容推荐等
- **关键概念**: 基于用户历史行为和相似用户/物品进行预测

### 4.2 使用每项特征
- **基于内容的推荐**:
  - 使用物品特征向量
  - 为用户学习参数，预测用户对未评分物品的评分  

为每个用户准备一个线性回归模型，推荐系统的代价函数是所有用户线性回归代价模型的加和
### 4.3 协同过滤算法
- **核心思想**: 同时学习用户参数和物品特征，考虑到实际情况中不会获得物品的特征信息，也不会获得到用户的偏好，只能获得一个评分表
- **优化目标**: $\min_{x^{(1)},...,x^{(n_m)}} \min_{\theta^{(1)},...,\theta^{(n_u)}} J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})$

### 4.4 二元标签
- **处理二元标签**: 喜欢/点击/收藏等二元反馈，利用逻辑回归模型sigmoid函数处理
- **修改代价函数**: 使用逻辑回归损失函数替代线性回归

## 第五部分 推荐系统进阶

### 5.1 均值归一化
- **目的**: 处理用户未评分任何物品的情况
- **方法**: 对每行（同一个物品/特征，而不是同一个人）减去平均评分μ，把这个数作为y来训练w,b，加快训练速度，最后用wx+b+μ来得到预测分，来作为一个未知用户的初始化参数


### 5.3 查找相关物品
- **方法**: 计算物品特征向量间的相似度  knn分类器(argmin(L2(xk-xi))^2)  L2距离的平方

协同过滤的局限：  
- 难以处理冷启动问题
- 对侧面信息利用度不足

## 第六部分 推荐系统高级主题

### 6.1 协同过滤 vs. 基于内容的过滤
- **协同过滤**: 利用用户行为数据，无需物品特征
- **基于内容**: 利用用户的特征，利用物品的属性，构建一个算法来确认他们是否匹配  
通过对用户特征处理得到一个向量v_ju,通过对物品属性处理得到一个向量v_im,两个v必须有相同的大小，最后计算向量点积，得到喜欢该电影的概率数字


### 6.2 基于内容过滤的深度学习
- **神经网络方法**: 使用神经网络学习用户和物品的表示，学习v_im和v_ju
- 代价函数 J=sigma{ (v_im*v_ju-y)^2 }
- 可以利用v_im来寻找相似物品  knn分类器(argmin(L2(v_im-v))^2)  L2距离的平方
- **优势**: 可以处理更复杂的特征和非线性关系

### 6.3 从大目录中推荐
- **挑战**: 计算所有物品的预测评分计算量大
- **解决步骤**:
  - 检索: 快速生成候选物品列表（可以根据用户历史观看，大多数人的喜爱，用户点击最多的类别来推荐）   
**确保广泛覆盖，即使出现了不喜欢的也没关系，增大检索量会优化算法，但会增大计算量**
  - 排名: 对候选物品进行精细评分  


### 6.4 推荐系统的道德使用
- **伦理考虑**:
  - 避免推荐有害内容
  - 减少信息茧房效应
  - 提高透明度
  - 保护用户隐私

## 第七部分 强化学习基础

### 7.1 什么是强化学习
- **核心概念**: 智能体通过与环境交互学习最优行为策略
- **关键要素**: 状态(State)、动作(Action)、奖励(Reward)
- (s,a,R(s,s'))
设立奖励函数或奖励机制，如果行为对就给正奖励，行为错误就给极大的负奖励
设立终端状态

### 7.3 强化学习中的回报
- **折现因子**: $\gamma$ (0≤γ≤1)，平衡即时与未来奖励
- **回报定义**: 累计奖励 $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...$

### 7.4 强化学习中的决策与策略制定
- **策略**: 从状态到动作的映射 $\pi(s) \rightarrow a$
- **目标**: 找到最大化期望回报的策略

### 7.5 回顾关键概念
- **总结**: 状态、动作、奖励、回报、策略、价值函数

## 第八部分 价值函数与Bellman方程

### 8.1 状态动作值函数定义
- **Q函数**: $Q(s,a)$ = 从状态s执行动作a后能获得的期望回报
- **最优Q函数**: $Q^*(s,a)$ = 遵循最优策略时的最大回报

### 8.2 状态动作值函数示例
- **示例分析**: 通过具体示例理解Q值的计算和意义

### 8.3 Bellman方程
- **公式**: $Q(s,a) = R(s) + \gamma \max_{a'} Q(s',a')$
- **意义**: 当前Q值与下一状态最大Q值之间的关系

### 8.4 随机环境
- **考虑不确定性**: 在随机环境中，状态转移具有概率性
- **修改Bellman方程**: 考虑期望值而非确定值

## 第九部分 深度强化学习

### 9.1 连续状态空间应用示例
- **挑战**: 状态空间过大或连续，无法使用表格表示
- **解决方案**: 使用函数近似（如神经网络）估计Q值

### 9.2 月球着陆器
- **环境介绍**: 控制登月器平稳降落在指定区域
- **状态空间**: 位置、速度、角度、角速度等连续值

### 9.3 学习状态值函数
- **深度Q网络(DQN)**: 使用神经网络近似Q函数
- **训练过程**: 最小化Bellman误差

### 9.4 算法优化-改进的神经网络结构
- **网络架构**: 输入状态，输出每个动作的Q值
- **改进方法**: 使用更深的网络结构捕捉复杂模式

### 9.5 算法优化ϵ-贪婪策略
- **探索-利用权衡**: 以ϵ概率随机探索，以1-ϵ概率选择最优动作
- **衰减ϵ**: 随时间逐步减少探索概率

### 9.6 算法优化-小批量和软更新（可选）
- **小批量学习**: 从经验回放缓冲区随机采样批量数据
- **软更新**: 缓慢更新目标网络参数，提高稳定性

### 9.7 强化学习的状态
- **状态表示**: 需要包含足够信息以做出最优决策
- **部分可观测性**: 在实际问题中，状态可能无法完全观测
