# 无监督学习
- 聚类
- 异常检测

## 2.1 什么是聚类
- **定义**: 将相似的数据点自动分组到簇中的无监督学习算法
- **特点**: 
  - 无标签数据
  - 寻找数据内在的分组结构
- **应用场景**:
  - 市场细分
  - 社交网络分析
  - 组织计算集群
  - 天文数据分析

## 2.2 K-means的直观理解
- **核心思想**: 
  - 随机初始化K个聚类中心  注意是随机！！！  
  - 迭代执行两个步骤:
    1. 分配步骤: 将每个点分配到最近的聚类中心
    2. 移动步骤: 将聚类中心移动到该簇所有点的均值位置
- **可视化**: 通过不断迭代，聚类中心逐渐移动到簇的"中心"位置

## 2.3 K-means算法
- **算法步骤**:
  1. 随机初始化K个聚类中心 μ₁, μ₂, ..., μₖ
  2. 重复直到收敛:
     - 对每个样本i，计算其所属簇: c⁽ⁱ⁾ = argminⱼ‖x⁽ⁱ⁾ - μⱼ‖²   (L2距离/欧几里得距离)
     **c⁽ⁱ⁾是该样本所分给类的编号，从1到k **
     - 对每个簇j，重新计算中心: μⱼ = (∑c⁽ⁱ⁾=j x⁽ⁱ⁾) / (c⁽ⁱ⁾=j的样本数)
    
  如果没有任何一个样本分配给某个类，直接删除该类，或重新初始化确保每个聚类都能分配到一些样本 

## 2.4 优化目标
- **代价函数**(失真函数): 
  J(c⁽¹⁾,...,c⁽ᵐ⁾, μ₁,...,μₖ) = (1/m)∑ᵢ₌₁ᵐ‖x⁽ⁱ⁾ - μ_c⁽ⁱ⁾‖²
  
  **注意J是距离函数的平方，这样做可以更好的反应聚类中心是否在整个簇的中心位置，如果只是不加平方的距离的话则无法确保在中心**  
- **优化目标**: 最小化代价函数J
- **算法保证**: 
  - 分配步骤: 固定μ，优化c，降低J
  - 移动步骤: 固定c，优化μ，降低J
  - 每次迭代都保证J单调递减  
成本函数停止下降或下降小于某个阈值时则已经收敛，成本函数绝对不可能上升

## 2.5 初始化K-means
- **随机初始化方法**:
  1. 从训练集中随机选择K(<m)个样本作为初始聚类中心
  2. 通常需要多次随机初始化(50-1000次)
  3. 选择代价函数J最小的结果(**因为使用了平方，所以J反映了类的集中程度**)
- **K值选择问题**: 随机初始化可能导致局部最优解

## 2.6 选择聚类的个数
- **肘部法则**(Elbow Method):
  - 绘制K值与代价函数J的关系曲线
  - 选择曲线"肘部"对应的K值(拐点)
  - **注意只选择使J最小的K是不起作用的，因为K越大成本函数越小（点越集中），所以会选择那个最大的K值**
- **实际考虑**:
  - 肘部法则可能不明显
  - 最终选择应该服务于后续目的
  - 根据不同需求评估不同K值的表现

## 第三部分 异常检测与推荐系统

### 3.1 发现异常事件
- **异常检测定义**: 识别与正常模式显著不同的数据点
- **应用场景**:
  - 欺诈检测
  - 工业制造缺陷检测
  - 数据中心监控异常机器
  - 医疗异常检测

### 3.2 高斯（正态）分布
- **公式**: $p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$
- **参数估计**:
  - $\mu = \frac{1}{m}\sum_{i=1}^m x^{(i)}$
  - $\sigma^2 = \frac{1}{m}\sum_{i=1}^m (x^{(i)}-\mu)^2$

### 3.3 异常检测算法
- **步骤**:
  1. 选择可能表示异常的特征$x_i$
  2. 计算参数$\mu_1,...,\mu_n,\sigma_1^2,...,\sigma_n^2$
  3. 对新样本$x$，计算$p(x) = \prod_{j=1}^n p(x_j;\mu_j,\sigma_j^2)$
  4. 如果$p(x) < \varepsilon$，则判定为异常

### 3.4 开发和评估异常检测系统
- **评估方法**:
  - 使用带标签数据（正常/异常）
  - 划分训练集（仅正常样本）、交叉验证集、测试集
  - 使用F1-score、精确率、召回率等指标

### 3.5 异常检测 vs. 监督学习
- **异常检测适用场景**:
  - 异常样本数量很少（0-20）
  - 异常类型多样，难以从正样本学习
- **监督学习适用场景**:
  - 正负样本数量都很大
  - 有足够多的异常样本用于训练

### 3.6 选择要使用的特征
- **特征转换**: 对非高斯分布特征进行转换（如log(x), x^c）
- **误差分析**: 添加新特征帮助区分异常值
- **多元高斯分布**: 可捕捉特征间的关系

## 第四部分 推荐系统基础

### 4.1 提出建议
- **推荐系统应用**: 电影推荐、产品推荐、内容推荐等
- **关键概念**: 基于用户历史行为和相似用户/物品进行预测

### 4.2 使用每项特征
- **基于内容的推荐**:
  - 使用物品特征向量
  - 为用户学习参数，预测用户对未评分物品的评分

### 4.3 协同过滤算法
- **核心思想**: 同时学习用户参数和物品特征
- **优化目标**: $\min_{x^{(1)},...,x^{(n_m)}} \min_{\theta^{(1)},...,\theta^{(n_u)}} J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})$

### 4.4 Binary labels- favs, likes and clicks
- **处理二元标签**: 喜欢/点击/收藏等二元反馈
- **修改代价函数**: 使用逻辑回归损失函数替代线性回归

## 第五部分 推荐系统进阶

### 5.1 均值归一化
- **目的**: 处理用户未评分任何物品的情况
- **方法**: 对每行（物品）减去平均评分

### 5.2 协同过滤的TensorFlow实现
- **自动微分**: 利用TensorFlow自动计算梯度
- **代码简化**: 无需手动实现梯度下降

### 5.3 查找相关物品
- **方法**: 计算物品特征向量间的相似度
- **相似度度量**: 余弦相似度 $sim(i,j) = \frac{x^{(i)} \cdot x^{(j)}}{||x^{(i)}|| ||x^{(j)}||}$

## 第六部分 推荐系统高级主题

### 6.1 协同过滤 vs. 基于内容的过滤
- **协同过滤**: 利用用户行为数据，无需物品特征
- **基于内容**: 需要物品特征，不依赖用户行为数据

### 6.2 Deep learning for content-based filtering
- **神经网络方法**: 使用神经网络学习用户和物品的表示
- **优势**: 可以处理更复杂的特征和非线性关系

### 6.3 从大目录中推荐
- **挑战**: 计算所有物品的预测评分计算量大
- **解决方案**:
  - 检索: 快速生成候选物品集合
  - 排名: 对候选物品进行精细评分

### 6.4 推荐系统的道德使用
- **伦理考虑**:
  - 避免推荐有害内容
  - 减少信息茧房效应
  - 提高透明度
  - 保护用户隐私

### 6.5 基于内容过滤的TensorFlow实现
- **实现要点**:
  - 双塔模型架构（用户塔和物品塔）
  - 使用点积或余弦相似度计算匹配分数

## 第七部分 强化学习基础

### 7.1 什么是强化学习
- **核心概念**: 智能体通过与环境交互学习最优行为策略
- **关键要素**: 状态(State)、动作(Action)、奖励(Reward)

### 7.2 火星探测器示例
- **示例说明**: 智能体在火星表面探索，最大化长期奖励
- **学习目标**: 找到最佳移动策略以最大化累计奖励

### 7.3 强化学习中的回报
- **回报定义**: 累计奖励 $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...$
- **折扣因子**: $\gamma$ (0≤γ≤1)，平衡即时与未来奖励

### 7.4 强化学习中的决策与策略制定
- **策略**: 从状态到动作的映射 $\pi(s) \rightarrow a$
- **目标**: 找到最大化期望回报的策略

### 7.5 回顾关键概念
- **总结**: 状态、动作、奖励、回报、策略、价值函数

## 第八部分 价值函数与Bellman方程

### 8.1 状态动作值函数定义
- **Q函数**: $Q(s,a)$ = 从状态s执行动作a后能获得的期望回报
- **最优Q函数**: $Q^*(s,a)$ = 遵循最优策略时的最大回报

### 8.2 状态动作值函数示例
- **示例分析**: 通过具体示例理解Q值的计算和意义

### 8.3 Bellman方程
- **公式**: $Q(s,a) = R(s) + \gamma \max_{a'} Q(s',a')$
- **意义**: 当前Q值与下一状态最大Q值之间的关系

### 8.4 随机环境（可选）
- **考虑不确定性**: 在随机环境中，状态转移具有概率性
- **修改Bellman方程**: 考虑期望值而非确定值

## 第九部分 深度强化学习

### 9.1 连续状态空间应用示例
- **挑战**: 状态空间过大或连续，无法使用表格表示
- **解决方案**: 使用函数近似（如神经网络）估计Q值

### 9.2 月球着陆器
- **环境介绍**: 控制登月器平稳降落在指定区域
- **状态空间**: 位置、速度、角度、角速度等连续值

### 9.3 学习状态值函数
- **深度Q网络(DQN)**: 使用神经网络近似Q函数
- **训练过程**: 最小化Bellman误差

### 9.4 算法优化-改进的神经网络结构
- **网络架构**: 输入状态，输出每个动作的Q值
- **改进方法**: 使用更深的网络结构捕捉复杂模式

### 9.5 算法优化ϵ-贪婪策略
- **探索-利用权衡**: 以ϵ概率随机探索，以1-ϵ概率选择最优动作
- **衰减ϵ**: 随时间逐步减少探索概率

### 9.6 算法优化-小批量和软更新（可选）
- **小批量学习**: 从经验回放缓冲区随机采样批量数据
- **软更新**: 缓慢更新目标网络参数，提高稳定性

### 9.7 强化学习的状态
- **状态表示**: 需要包含足够信息以做出最优决策
- **部分可观测性**: 在实际问题中，状态可能无法完全观测
