# 强化学习的数学原理 - 课程笔记

//**课程地址：** [B站 - 强化学习的数学原理](https://www.bilibili.com/video/BV1r3411Q7Rr/?spm_id_from=333.1387.collection.video_card.click&vd_source=bbf7874c85687ec5fb9348883f1a1b37)


## 第1课 - 基本概念

### Part 1: State, Action, Policy

#### 1. 核心思想
- **强化学习 (Reinforcement Learning, RL)**：智能体（Agent）通过与环境（Environment）交互，基于获得的奖励（Reward）来学习最佳行为策略（Policy），以最大化**长期累积奖励**。
- **类比**：训狗。狗（Agent）通过尝试动作（如坐下），获得零食（Reward），最终学会为了获得最多零食而采取的行动策略。

#### 2. 核心名词
- **状态 (State, `s`)**：
  - 描述环境当前情况的信息，是智能体决策的依据。
  - *例如：围棋棋盘上的棋子布局、自动驾驶汽车传感器数据。*

- **动作 (Action, `a`)**：
  - 智能体在某个状态下可以采取的行为。
  - *例如：机器人选择移动方向、推荐系统选择要推荐的物品。*
  - 所有可能动作的集合称为动作空间（Action Space）。

- **策略 (Policy, `π`)**：
  - 智能体的“大脑”或行为函数。它规定了在某个状态下应该采取什么动作。
  - 通常表示为一个概率分布 `π(a|s)`，即在状态 `s` 下选择动作 `a` 的概率。
  - **目标**：找到最优策略 `π*`，从而获得最大的长期回报。

### Part 2: Reward, Return, MDP

- **奖励 (Reward, `R`)**：
  - 环境在智能体执行一个动作后提供的即时反馈信号（标量值）。
  - *例如：游戏得分增加/减少、自动驾驶中安全行驶（正奖励）或发生碰撞（负奖励）。*
  - **RL的核心**：奖励定义了问题的目标。

- **回报 (Return, `G`)**：
  - **长期累积奖励**，是智能体真正需要最大化的目标。
  - 引入了**折扣因子 (Discount Factor, `γ`)** 的概念（`γ ∈ [0, 1]`），用于权衡即时奖励和未来奖励的重要性。
  - **折扣回报 (Discounted Return)** 公式：
    $$G_t = R_{t+1} + γR_{t+2} + γ^2R_{t+3} + ... = \sum_{k=0}^{\infty} γ^k R_{t+k+1}$$
  - `γ` 接近 0：智能体是“短视的”，更关心即时奖励。
  - `γ` 接近 1：智能体是“有远见的”，更关心长期奖励。

- **马尔可夫决策过程 (Markov Decision Process, MDP)**：
  - 强化学习问题的标准化数学模型，几乎所有的RL问题都可以用MDP来表述。
  - **一个MDP由五元组构成：`(S, A, P, R, γ)`**
    - `S`：状态（State）的集合。
    - `A`：动作（Action）的集合。
    - `P`：状态转移概率（Transition Probability），`P(s'|s, a)`。表示在状态 `s` 下执行动作 `a` 后，转移到状态 `s'` 的概率。**具有马尔可夫性**（下一状态只取决于当前状态和动作，而与历史无关）。
    - `R`：奖励函数（Reward Function），通常 `R(s, a)` 或 `R(s, a, s')`。
    - `γ`：折扣因子。

---

## 第2课 - 贝尔曼公式 (Bellman Equation)

### Part 1: 例子说明Return的重要性
- 通过简单例子对比了只考虑即时奖励（Myopic）和考虑长期回报（Forward-looking）策略的不同。
- **结论**：好的策略不能只看一步的即时奖励，必须评估动作带来的长期后果（Return）。

### Part 2: State Value 的定义
- **状态值函数 (State-Value Function, `v_π(s)`)**：
  - 衡量一个状态 `s` 的长期价值。
  - 定义：**从状态 `s` 开始，遵循策略 `π` 所能获得的期望回报（Expected Return）。**
    $$v_π(s) = E_π[G_t | S_t = s]$$
  - 值越高，说明从该状态出发的未来前景越好。
  - **注意**：价值函数依赖于所选择的策略 `π`，不同策略下同一状态的价值不同。

### Part 3: 贝尔曼公式的详细推导
- **贝尔曼公式 (Bellman Equation)**：
  - 描述了状态价值函数 `v_π(s)` 自身的递归关系，是强化学习中最核心的方程。
  - **推导核心**：将当前的即时奖励 `R_{t+1}` 和下一个状态 `s'` 的折扣价值 `γv_π(s')` 结合起来。
  - **公式**：
    $$v_π(s) = \sum_{a} π(a|s) \sum_{s'} P(s'|s, a) [ R(s, a, s') + γ v_π(s') ]$$
  - **解读**：状态 `s` 的价值 = 所有可能动作的概率 × (该动作导致所有可能转移的) [即时奖励 + 下一个状态的折扣价值] 的期望和。

### Part 4: 公式向量形式与求解
- **贝尔曼公式的矩阵形式**：
  - 将所有状态的价值 `v_π(s)` 写成向量形式 `v`。
  - 将奖励和状态转移信息写入矩阵。
  - 公式可重写为：`v = r + γP v`
    - 其中 `P` 是状态转移矩阵，`r` 是期望即时奖励向量。
- **求解状态价值**：
  - 贝尔曼方程是一个线性方程组。
  - 可通过直接求解矩阵方程得到解析解：`v = (I - γP)^{-1} r`
    - （其中 `I` 是单位矩阵）
  - 对于大规模问题，通常使用迭代法（如动态规划、蒙特卡洛、时序差分）来近似求解。

### Part 5: Action Value 的定义
- **动作值函数 (Action-Value Function, `q_π(s, a)`)**：
  - 也称为 **Q函数 (Q-Function)**，是许多著名算法（如Q-Learning、DQN）的基础。
  - 定义：**在状态 `s` 下，选择动作 `a` 并之后始终遵循策略 `π` 所能获得的期望回报。**
    $$q_π(s, a) = E_π[G_t | S_t = s, A_t = a]$$
  - **与状态值函数 `v_π(s)` 的关系**：
    - 状态价值是所有动作价值的期望：`v_π(s) = ∑ π(a|s) * q_π(s, a)`
    - 动作价值是即时奖励加上后续状态价值的期望：`q_π(s, a) = ∑ P(s'|s, a) [ R + γ v_π(s') ]`
  - **重要性**：`q_π(s, a)` 直接评估了在特定状态下某个特定动作的好坏，比 `v_π(s)` 更直接地用于改进策略。
