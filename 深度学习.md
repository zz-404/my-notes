## 神经网络
- 神经元  
   获取输入数字，给出输出数字（激活值），输出可继续作为其他神经元的输入
- 层  
   一层可以有一个或多个神经元，接受上一层的输入向量，为下一层提供输出向量

**某一层的神经元将可以访问输入层中的每一个特征**    


**神经网络所做的是：  
不需要手动设置特征，隐藏层可以自己学习一个更加合适的特征**

多层感知器，拥有多个层的神经网络

## 神经网络中的层
输入层被称为第0层  
使用上标方括号索引某个层，下标索引神经元
$$a_j^{(i)}=g( w_j^{(i)}*a^{(i-1)}+b_j^{(i)} )$$

前向传播：输入x从第0层开始传播到输出层输出y，用于推理，而反向传播可用于**学习**

```python
x=np.array([1,2,3])
layer1=Dense(units=3,activation='sigmoid')
a1=layer1(x)
layer2=Dense(units=1,activation='sigmoid')
a2=layer1(a1)
```
利用tensorflow构建神经网络
```python
import tensorflow
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
x=x=np.array([1,2,3])
y=np.array([1,1,0])

layer1=Dense(units=3,activation='sigmoid')
layer2=Dense(units=1,activation='sigmoid')
model = Sequential ([layer1,layer2])
#or   model = Sequential ([Dense(units=3,activation='sigmoid'),Dense(units=1,activation='sigmoid')])

model.compile(...)
model.fit(x,y)
model.predict(newx)
```

## 前向传播的一般实现
```python
def dense(a_in,W,b,g):
   #a_in输入值
   #W是各个神经元参数的堆叠，每个列向量表示一个神经元中的参数
   #b是各个神经元偏置的堆叠，每一列表示一个神经元的偏置
   a_out=a_in @ W +b
   return g(a_out)

def sequntial(x):
   a1=dense(x,W1,b1,g)
   a2=dense(a1,W2,b2,g)
   a3=dense(a2,W3,b3,g)
   return a3
```

## 训练神经网络
- 第一步 架构神经网络及选择合适的计算模型  
   model = Sequential([])
- 第二步 确定损失函数  
   model.compile(loss=)
- 第三步 最小化损失函数  
   model.fit(X,y,epoches=1000)
```python
import tensorflow
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model = Sequential ([Dense(units=3,activation='sigmoid'),Dense(units=1,activation='sigmoid')])

from tensorflow.keras.losses import BinaryCrossentropy
#二元交叉熵损失函数，适用于二分类问题，实际上与逻辑回归的损失函数相同
model.compile(loss= BinaryCrossentropy() )
#MeanSquaredError() 平方误差损失函数

model.fit(X,y,epoches=1000)
#fit函数通过反向传播来计算偏导数项，因此不需要手动计算
```

### sigmoid的替代函数
- ReLU函数(校正线性单元)  
   g(z)=max( 0 , z )
- 线性激活函数
   g(z)= z

二进制分类问题 sigmoid  
有正负的回归问题可以用 linear（只使用线性回归函数将只能拟合线性回归模型，失去非线性能力）    
无负数的回归 ReLU  

**RELU最常见的激活函数**
- 计算效率高
- 只在一个地方变平，变平会减慢学习速度


## 多类
### softmax
$$z_j = 
