# 具身智能

## 具身智能是什么
- 环境理解，智能交互，认知推理，规划执行于一体的系统化方案
- LLM桥接人类指令意图到具体规划代码实现

## 基础技术路线
- 场景理解 
  - 检测分割 
    - 检测SAM，SAM3d
    - 分割open voc detection
  - 多模态grounding
    - 将抽象与具体建立联系的过程
- 数据引导
  - 从视频
  - 硬件从现实生活采集
  - 生成式采集
  
- 动作执行
- 世界模型


**Transformer:**  
是一种基于自注意力机制的深度学习模型架构,通过自回归，根据上文预测下一个最可能的词，从而生成流畅的文本。       
让模型能够直接关注到输入序列中的所有部分，从而更好地理解上下文。  
主要优势：
1. 强大的并行计算能力，训练速度快。
2. 卓越的长距离依赖建模能力，处理长文本效果极佳。  
为什么重要	它是当今所有主流大语言模型（如GPT系列）的构建基石，推动了生成式AI的革命。
# 视觉基础模型

## 1.1 CLIP (Contrastive Language-Image Pre-training) - OpenAI

### 核心原理
**对比学习框架**：CLIP通过对比学习方式，在一个大规模图文对数据集上训练，学习图像和文本的联合表示空间。模型将图像和文本编码到同一向量空间，通过计算余弦相似度衡量匹配程度。最基本的应用是可以计算图像与语言描述的相似度

### 技术特点
- 双编码器架构：包含图像编码器和文本编码器
- 大规模预训练：使用4亿个图文对进行训练
- 零样本能力：无需微调即可应用于新的视觉概念
- 多模态理解：同时理解视觉和语言信息

**Zero-Shot学习：在训练集中没有某个类别的样本，但在测试集中出现了这个类别。我们需要模型在训练过程中，即使没有接触过这个类别的样本，但仍然可以通过对这个类别的描述，对没见过的类别进行分类。**  

**One-Shot学习：可以理解为用一条数据fine-tune模型。例如，在人脸识别场景里，你只提供一张照片，门禁就能认识各个角度的你。属于Few-Shot学习的特例。**  

**Few-Shot学习：在模型训练过程中，如果每个类别只有少量样本（一个或几个），研究人员希望机器学习模型在学习了一定类别的大量数据后，对于新的类别，只需要少量的样本就能快速学习。**  

### 具体应用场景
1. 零样本图像分类：通过文本提示进行分类
2. 图像检索：基于文本描述搜索相关图像
3. 特征提取：中间层特征用于下游任务微调
4. 质量评估：评估生成图像与文本描述的匹配度

**优势**：
- 强大的泛化能力
- 支持开放词汇识别
- 良好的零样本性能

**局限**：
- 对细粒度细节敏感度不足
- 计算资源需求较大
- 可能存在社会偏见

## 1.2 DINO/DINO-v2 - Meta

### 核心创新
**自监督学习机制**

### 技术细节
- 无标签学习：完全不需要人工标注

### DINO-v2改进
1. 更大规模训练：使用6.42亿张图像
3. 更好的正则化：减少过拟合风险
4. 多分辨率支持：适应不同输入尺寸

### 应用价值

- **特征对应**：**发现不同图像中的相似结构**

- **语义分割**：提供高质量像素级特征
- **目标检测**：作为backbone提升检测性能
- **图像检索**：强大的特征表示能力

## 1.3 SAM/SAM2 - Meta

### SAM (Segment Anything Model)
**提示驱动的分割**：接受点、框、文本等提示，生成高质量分割掩码。

**技术特点**：
- **大规模训练**：1100万张图像，10亿个掩码
- **三组件架构**：图像编码器、提示编码器、掩码解码器
- **零样本泛化**：无需训练即可处理新图像
- **实时性能**：50ms内完成分割

### SAM2 升级特性
1. **视频分割**：支持时序一致性分割

3. **长序列处理**：**处理长时间视频序列**

5. **记忆机制**：保持跨帧的对象一致性
6. **效率优化**：更高的处理速度

### 应用场景
- **图像编辑**：精确的对象选择和分离
- **视频处理**：视频对象分割和追踪
- **科研工具**：生物图像分析、遥感图像处理
- **AR/VR**：实时场景理解和交互

## 1.4 Grounding-DINO - IDEA研究院  
**Marrying DINO with Grounded Pre-Training**  

**can detect arbitrary objects with human inputs such as category names or referring expressions.**  

### 独特优势
**零样本检测 ：无需训练即可检测新类别**

### 应用领域
- **智能监控**：基于描述的目标查找
- **内容审核**：特定内容检测和过滤
- **机器人视觉**：自然语言指令理解
- **图像标注**：自动化标注生成


## 1.5 OmDet-Turbo - OmAI Lab

### 性能优化
**高效架构设计**：专为实时应用优化的开放词表检测模型。
**高效融合头，一个快速的多模态融合模块，旨在减轻编码器的计算负担并减少具有ROI的头部的耗时。**
  优势：速度极快

## 1.6 Grounded-SAM

### 集成方案
**检测+分割流水线**：结合Grounding-DINO的检测能力和SAM的分割精度。

**工作流程**：
1. **文本引导检测**：使用Grounding-DINO定位目标
2. **提示生成**：从检测结果生成分割提示
3. **精细分割**：SAM生成精确掩码
4. **后处理优化**：结果 refinement

### 应用价值
- **实例分割**：文本指导的精确分割
- **交互式编辑**：自然语言驱动的图像编辑
- **数据标注**：自动化标注工具
- **研究平台**：多模态视觉研究基础

## 1.7 FoundationPose - NVIDIA

### 技术突破
**6D姿态估计**：统一框架处理已知和未知物体的姿态估计和追踪。

**核心特性**：
- **零样本能力**：无需训练即可处理新物体
- **多模态输入**：支持RGB、RGB-D、点云输入
- **实时追踪**：高帧率的姿态更新
- **鲁棒性能**：应对遮挡和光照变化

### 应用领域
- **机器人抓取**：精确的物体定位
- **AR/VR**：真实世界物体追踪
- **质量检测**：工业零件位姿检测
- **自动驾驶**：周围环境感知

## 1.8 Stable Diffusion

## 1.9 Depth Anything - 港大&字节

### 技术特色
**单目深度估计**：从单张RGB图像估计深度信息。

**版本对比**：
- **v1**：基础版本，良好的泛化能力
- **v2**：改进版本，更高的精度和稳定性

### 核心创新
- **大规模训练**：150万张图像训练
- **多任务学习**：结合多个深度估计任务
- **高效架构**：实时推理能力
- **强泛化性**：适应各种场景

## 1.10 Point Transformer v3

### 点云处理
**新一代点云架构**：专门为点云数据设计的Transformer架构。

**技术优势**：
- **置换不变性**：适应点云无序特性
- **局部特征聚合**：有效的邻域信息提取
- **可扩展性**：支持大规模点云
- **多任务支持**：分类、分割、检测等任务

## 1.11 RDT-1B - 清华

### 机器人专用模型
**双臂操作基础**：专门为机器人操作任务设计的大模型。

- **few-shot学习**：少量演示即可学习新任务
- **多模态输入**：视觉、语言、传感器融合
- 
## 1.12 SigLIP

### CLIP改进版本
**Sigmoid损失函数**：使用sigmoid交叉熵损失替代softmax损失。  

SigLIP是一种类似于CLIP的多模态图像-文本模型。它使用独立的图像和文本编码器来生成两种模态的表示。与CLIP不同，SigLIP在训练过程中对图像-文本对使用成对sigmoid损失。这种训练损失消除了在批次内对所有图像和文本成对相似性的全局视图的需求。因此，它使得在更大的批次大小上更有效地扩展，同时也能在较小的批次大小上提供更优越的性能。您可以在SigLIP收藏夹下找到所有原始的SigLIP检查点。
**改进点**：
- **训练稳定性**：更稳定的训练过程
- **计算效率**：降低计算复杂度
- **性能提升**：在某些任务上表现更好
- **易于微调**：更好的迁移学习能力


## 3.视觉-语言-动作模型 (VLA Models)  
Vision-Language-Action Models(VLA模型) 是一种结合VLM(Vision-Language Model)与机器人控制的模型，旨在将预训练的VLM直接用于生成机器人动作(RT-2中定义)。和以往利用VLM做planning以及build from strach的方法不同，VLA无需重新设计新的架构，将动作转化为token，微调VLM。  
### 3.1 基本概念
- **定义**: 结合VLM与机器人控制的模型
- **特点**: 
  - 端到端学习
  - 使用LLM/VLM骨干网络
  - 加载预训练模型
  - 动作token化表示

### FAST tokenizer
受到llama中使用的 BPE 编码方法的启发，在模型训练之前压缩机器人动作信号，减少连续token之间的相关性。考虑到机器人动作是连续信号，因此采用离散余弦变换编码，由此产生的 tokenization 方法称为 Frequency-space Action Sequence Tokenization (FAST)。
                        
原文链接：https://blog.csdn.net/weixin_39653948/article/details/145371881


### 3.2 分类维度
1. 模型结构和大小: action head设计, tokenize方法
2. 预训练与微调策略: 数据集选择和处理
3. 输入输出格式: 2D vs. 3D, Visual Trace输入
4. 应用场景: 不同机器人任务适配

### VLA 经典方案
1.经典方案用encoder-decoder型Transformer或类似的结构from scratch训练，把机器人状态和视觉观测当成潜在条件，然后用action query-based Transformer decoder解码出动作。  

2.拥抱预训练的 LLM/VLM，把action当成token直接预测，好处是借鉴成熟的语言模型，预训练和scale的经验成本更小。  

3.拥抱扩散模型Diffusion Model，Diffusion多步降噪对于fine-grained灵巧任务更加友好    

**扩散模型的工作原理！训练阶段（第1&2步）： 学习“如何从任何程度的噪音图中还原出干净图”。  
生成/推理阶段（第3步）： 从纯噪音开始，多次调用学到的“去噪”本领，一步步“幻想”出全新的图像。** 
多步降噪对于fine-grained灵巧任务更加友好 正是因为这种迭代、渐进、可微调的特性，让它非常适合生成那些需要“手感”和“巧劲”的精细动作，容错性更高，控制也更精准。    

4.LLM + Diffusion：LLM用来压缩多模态表征，Diffusion作为action expert精细化输出action trajectories。  

5.视频生成 + 逆运动学：参考World Model的一些发展，先根据首帧和指令生成运动视频，然后逆运动学得到对应的动作。这种方式可解释性很强，但是非常受到视频生成质量和instruction-following的能力的影响。【虽然难，但是scope会更大，例如UniSim可以生成corner case训练数据，还可以当成闭环仿真环境来琢磨RL算法】


### 3.3经典工作

#### 3.3.1 自回归模型
- **RT系列** (Google Deepmind):
  - RT-1: 基础机器人transformer
  - RT-2: 55B参数大规模模型
  - RT-Trajectory: 轨迹生成
  - AUTORT: 自动机器人transformer
